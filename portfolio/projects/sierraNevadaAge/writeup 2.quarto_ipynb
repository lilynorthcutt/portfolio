{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Geologic Magmatic Migration Exploration Through Age Prediction\"\n",
        "output: github_document\n",
        "categories: [Python, GIS, Shiny, EDA, ML]\n",
        "date: \"2024-07-22\"\n",
        "description: \"Developping novel ML methods for magamtic migration predictions\"\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Access the interactive dashboard of the data and predictions [here](https://iawc6a-lily-northcutt.shinyapps.io/geoAges/) !**\n",
        "\n",
        "**Access the notebook for the full code and predictions at the [github repository](https://github.com/lilynorthcutt/sierraNevadaAge) !**\n",
        "\n",
        "### Summary\n",
        "\n",
        "This project aims to deepen our understanding of magmatic migration within the Sierra Nevadas. The current samples of geologic ages do not provide uniform coverage of the area. This project aims to offer a solution to this problem by exploring the success of various machine learning (ML) algorithms to predict geologic ages within the region.\n",
        "\n",
        "In this write-up, I will go through my process for:\n",
        "\n",
        "-   Data Ingestion, Cleaning, and Wrangling\n",
        "-   Exploratory Data Analysis (EDA)\n",
        "-   Feature Engineering\n",
        "-   Training Models\n",
        "    -   Building custom Cross-Validation pipeline\n",
        "    -   Fine Tuning Parameters\n",
        "-   Model Evaluation\n",
        "\n",
        "> Please note, this project is ongoing, and ever evolving. Currently (July 22) I am working on using the models to predict unknown points for visualizations in the Shiny App.\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Magmatism plays a key role in mountain formation, as ascending magmas add mass and volume to the Earth's surface and subsurface. However, the causes and rates of magma movement remain largely unknown, necessitating a deeper understanding of these processes. The mountains and canyons in the Sierra Nevadas are formed from granitic rocks, which originated from molten rock that cooled far beneath the Earth's surface. By examining the geologic ages within this area, we can gain insights into the age of the rocks at various locations and understand the magmatic migration and processes that created them.\n",
        "\n",
        "The current data in the Sierra Nevadas does not uniformly cover the area, making it unrepresentative of the entire region. This is a common problem in geology, as uniformly collecting samples across a region is often impractical due to constraints such as time, resources, coverage area, or sample accessibility. Therefore, there is a need for novel methods to mitigate sampling bias and use geologic age samples to predict uniform coverage of the area.\n",
        "\n",
        "**Key Project Goals:**\n",
        "\n",
        "The key goals this project aims to address are the following:\n",
        "\n",
        "-   **Develop novel machine learning (ML)** methods to fill gaps in geologic maps by predicting undated rock ages.\n",
        "-   **Apply the results from the model to predict** undated rock ages, providing a less biased view of magmatic migration.\n",
        "-   **Build Interactive Visualizations** for researchers and curious minds to view and explore the data and predictions.\n",
        "\n",
        "This project was done with `python` for all data cleaning, model training/evaluation, and predictions, and `R` for interactive visualizations in Shiny. he full code is extensive, so some parts are omitted here for brevity. Please refer to the link at the top to view the notebook with the entire code.\n",
        "\n",
        "Let's begin!\n",
        "\n",
        "## Preprocessing\n",
        "\n",
        "We have two data types: a csv file, and shapefiles. The csv file contains ages at specific locations, while the shapefiles contain a boundary of interest and polynomials with their geologic period.\n",
        "\n",
        "### Load Data\n"
      ],
      "id": "f439fed4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import utm\n",
        "import time\n",
        "from os.path import join\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from ipywidgets import widgets\n",
        "from IPython.display import display\n",
        "\n",
        "import geopandas as gpd\n",
        "from pyhigh import get_elevation_batch\n",
        "from shapely.geometry import Point, LineString, Polygon\n",
        "from shapely.prepared import prep\n",
        "\n",
        "\n",
        "# Modeling\n",
        "from sklearn.model_selection import train_test_split, KFold, GridSearchCV, BaseCrossValidator\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic, ExpSineSquared, DotProduct, ConstantKernel as C\n",
        "from sklearn.inspection import permutation_importance\n",
        "#from pykrige.ok import OrdinaryKriging \n",
        "import pickle\n",
        "from prettytable import PrettyTable \n",
        "\n",
        "#%%\n",
        "random.seed(1)"
      ],
      "id": "6ab7a542",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Read in CSV file\n",
        "csv_file_path = \"Data/raw/attia_CentralSierraCretaceousIntrusionsGeochronologyData_forLily12082023.xlsx\"\n",
        "df_ages_raw = pd.read_excel(csv_file_path, sheet_name =  'CentralSierraNevada Rock Ages')\n",
        "\n",
        "df_ages_raw.head(5)"
      ],
      "id": "46eab82d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Read in Shapefiles\n",
        "# Read in shapefiles\n",
        "\n",
        "def read_shapefiles_in_folder(folder_path):\n",
        "    'FUNCTION TO READ IN ALL SHAPEFILES FROM A FOLDERPATH'\n",
        "    shapefiles_dict = {}\n",
        "\n",
        "    # Check if the folder path exists\n",
        "    if not os.path.exists(folder_path):\n",
        "        raise FileNotFoundError(f\"The folder '{folder_path}' does not exist.\")\n",
        "\n",
        "    # Iterate through files in the folder\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".shp\"):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            # Extract file name without extension\n",
        "            file_name = os.path.splitext(filename)[0]\n",
        "            # Read shapefile into GeoDataFrame\n",
        "            gdf = gpd.read_file(file_path)\n",
        "            # Store GeoDataFrame in the dictionary\n",
        "            shapefiles_dict[file_name] = gdf\n",
        "            \n",
        "    return shapefiles_dict\n",
        "\n",
        "folder_path = 'Data/raw/CentralSierraMapData_ForLily20231220'\n",
        "\n",
        "try:\n",
        "    # Read shapefiles into a dictionary\n",
        "    shapefiles_data = read_shapefiles_in_folder(folder_path)\n",
        "\n",
        "    # Access GeoDataFrames by file name\n",
        "    for file_name, gdf in shapefiles_data.items():\n",
        "        print(f\"File Name: {file_name}\")\n",
        "        #print(gdf.head())  # Display the first few rows of the GeoDataFrame\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(e)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "id": "bb681197",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are interested in 2 of the shapefiles:\n",
        "\n",
        "-   DetailedMapDataAreaBoundaryLine: Provides boundary linestring of the area of interest\n",
        "-   CretaceousIntrusionsIndividualPolygons: Provides polygon geometry with associated geologic time period of the are\n",
        "\n",
        "### Data Cleaning and Wrangling\n",
        "\n",
        "To ensure the data is ready for analysis, we will **handle missing values**, and add geospatial characteristics (POINT() shape). Additionally, lets add some approximate ages to the periods in the shapefiles (late Cretaceous, early Cretaceous, and Jurassic Cretaceous), and then combine the shapefiles with like periods.\n"
      ],
      "id": "220a18a5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Clean csv\n",
        " \n",
        "# ------ Handle missing values ---------\n",
        "# When original name is NA, then originalName == AnalysisName\n",
        "df_ages_raw['OriginalName'] = df_ages_raw['OriginalName'].fillna(df_ages_raw['AnalysisName']) \n",
        "# When easting & northing == \"NA*\" then convert to NA\n",
        "df_ages_raw['Easting'] = pd.to_numeric(df_ages_raw['Easting'].replace('NA*', pd.NA), errors = 'coerce') \n",
        "df_ages_raw['Northing'] = pd.to_numeric(df_ages_raw['Northing'].replace('NA*', pd.NA), errors = 'coerce')\n",
        "# Remove when location (easting or northing) is NA because location of age is a must have\n",
        "df_ages = df_ages_raw[pd.notna(df_ages_raw['Easting']) & pd.notna(df_ages_raw['Northing'])].copy()\n",
        "print(\"Size of cleaned csv data:\", len(df_ages))\n",
        "\n",
        "# ---------- Create POINT dataframe -----------\n",
        "# Make points a POINT() geometry\n",
        "# EPSG:26711\n",
        "df_point = gpd.GeoDataFrame(df_ages, geometry=gpd.points_from_xy(df_ages.Easting, df_ages.Northing),  crs='epsg:26711')\n",
        "\n",
        "#### Get age statistics\n",
        "print(df_ages['Age'].describe())\n",
        "\n",
        "##### View distrubtion of ages\n",
        "plt.hist(df_ages['Age'], bins = 40)\n",
        "plt.xlabel(\"Ages\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Distribution of Ages over Entire Area\")\n",
        "plt.figure(figsize=(.5, .5), dpi=80)\n",
        "plt.show()"
      ],
      "id": "5a81691f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Clean shapefile\n",
        "\n",
        "# ------------- Add Numeric Approx Age ---------------\n",
        "#shapefiles_data[\"CretaceousIntrusionsIndividualPolygons\"].Age.unique()\n",
        "#array(['eK', 'KJ', 'lK', 'eK?', 'lK?'], dtype=object)\n",
        "# PERIODS:\n",
        "# J: 201.3-145.0 MYA\n",
        "# K: 145.0-66.0 MYA\n",
        "# Thus mapping eK to mean(145 and mean(145, 66)), lK to mean(mean(145, 66), 66) and KJ to 145\n",
        "# Note that the oldest point in df_ages is 124.2\n",
        "k_mean = (145+66)/2 ; eK_mean = (145+k_mean)/2; lk_mean = (k_mean+66)/2; kJ = 145;\n",
        "(shapefiles_data[\"CretaceousIntrusionsIndividualPolygons\"])[\"ageNum\"] = shapefiles_data[\"CretaceousIntrusionsIndividualPolygons\"].Age.apply(\n",
        "    lambda x: eK_mean if (x == 'eK' or x == 'eK?') else(lk_mean if (x == 'lK' or x == 'lK?') else kJ))\n",
        "    \n",
        "# Number of polygons in each shapefile:\n",
        "for file, gdf in shapefiles_data.items():\n",
        "    print(f\"File: {file}   Number of shapes: {len(gdf)}\")\n",
        "\n",
        "##### View distrubtion of approximate ages\n",
        "plt.hist((shapefiles_data[\"CretaceousIntrusionsIndividualPolygons\"])[\"ageNum\"], bins = 40)\n",
        "plt.xlabel(\"Ages\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Count of Approximate Ages of Polygons\")\n",
        "plt.figure(figsize=(.5, .5), dpi=80)\n",
        "plt.show()\n"
      ],
      "id": "bdafb3b7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are over 1400 shapes in our individual polygon file (with periods), but only 3 periods. For convenience we will combine polygons if they touch and have the same period.\n"
      ],
      "id": "8f4b0d96"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------- Combined if polygon touching and same period ----------\n",
        "shapefiles_data['UnionPolygons'] = shapefiles_data[\"CretaceousIntrusionsIndividualPolygons\"].dissolve(by = 'Age')\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,10))\n",
        "legend_criteria = \"ageNum\"\n",
        "ax[0].set_title(f'Individual Polygons')\n",
        "ax[1].set_title(f'Union of Polygons')\n",
        "\n",
        "shapefiles_data[\"CretaceousIntrusionsIndividualPolygons\"].plot(figsize=(10, 10), alpha=0.5, edgecolor='k',column=legend_criteria, cmap='viridis', legend=False,ax=ax[0] )\n",
        "shapefiles_data['UnionPolygons'].plot(figsize=(10, 10), alpha=0.5, edgecolor='k',column=legend_criteria, cmap='viridis', legend=False,ax=ax[1] )\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "id": "95aec010",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***Note 1:*** The union creates 6 shapefiles because some of the periods are more uncertain and have a `?`. For now, we are handling all periods as if they are all accurate.\n",
        "\n",
        "***Note 2:*** There are only polynomials within our boundary.\n",
        "\n",
        "***Note 3:*** The union creates a shape called a \"MULTIPOLINOMIAL\" which we can ultimately handle the same as a normal POLYNOMIAL() geometry.\n",
        "\n",
        "### Exploratory Data Analysis\n",
        "\n",
        "Now that we have the data, and it is cleaned up, we can start visualizing our data and exploring it. We already see that our individual ages are mostly between 90-98 Million year ago (Ma), with average age of 99 Ma. Additionally, our polygons reference 3 periods. Note that while the above graph counts the number of polygons it doesn't take into account the total area of the polygons. As we will see below the oldest polygons take up a very small amount of the area.\n",
        "\n",
        "Let's first take a look at our map:\n"
      ],
      "id": "6125b66e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from branca.colormap import linear\n",
        "# In order to color on the same scale we need to normalize ages between the two data sources\n",
        "# and create a color scale\n",
        "min_age = min(min(df_point[\"Age\"]),min(shapefiles_data[\"UnionPolygons\"][\"ageNum\"]))\n",
        "max_age = max(max(df_point[\"Age\"]),max(shapefiles_data[\"UnionPolygons\"][\"ageNum\"]))\n",
        "colormap = linear.YlOrRd_09.scale(min_age, max_age)\n",
        "\n",
        "# IDs for each polygon in each shapefile\n",
        "(shapefiles_data[\"DetailedMapDataAreaBoundaryLine\"])[\"id\"] = shapefiles_data[\"DetailedMapDataAreaBoundaryLine\"].index.astype(str)\n",
        "# IDs for each polygon in each shapefile\n",
        "(shapefiles_data[\"UnionPolygons\"])[\"id\"] = shapefiles_data[\"UnionPolygons\"].index.astype(str)"
      ],
      "id": "f2366731",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interact with me!**\n"
      ],
      "id": "d36a7a31"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | include: false\n",
        "import folium\n",
        "\n",
        "# Initialize map\n",
        "interactive_map = folium.Map(\n",
        "    location=(37.75, -119.8),\n",
        "    zoom_start=9.25,\n",
        "    control_scale=True,\n",
        "    tiles=\"Esri.WorldShadedRelief\"\n",
        ")\n",
        "\n",
        "# Boundary Layer\n",
        "boundary_layer = folium.GeoJson(data=shapefiles_data[\"DetailedMapDataAreaBoundaryLine\"][[\"geometry\", \"id\"]],\n",
        "                               style_function=lambda feature: {\n",
        "                                   \"color\": \"black\",\n",
        "                                   \"weight\": 5,\n",
        "                                   \"dashArray\": \"5, 20\"\n",
        "                               }).add_to(interactive_map)\n",
        "                               \n",
        "\n",
        "# Create Popup template for locations\n",
        "popup_shp = folium.GeoJsonPopup(\n",
        "    fields=[ \"ageNum\"],\n",
        "    aliases=[\"Approx. Age:\"],\n",
        "    localize=True,\n",
        "    labels=True,\n",
        "    style=\"background-color: yellow;\",\n",
        ")\n",
        "\n",
        "tooltip_shp = folium.GeoJsonTooltip(\n",
        "    fields=[ \"ageNum\"],\n",
        "    aliases=[\"Approx Age: \"],\n",
        "    localize=True,\n",
        "    sticky=False,\n",
        "    labels=True,\n",
        "    style=\"\"\"\n",
        "        background-color: #F0EFEF;\n",
        "        border: 2px solid black;\n",
        "        border-radius: 3px;\n",
        "        box-shadow: 3px;\n",
        "    \"\"\",\n",
        "    max_width=800,\n",
        ")\n",
        "\n",
        "# Individual polygon layer\n",
        "folium.GeoJson(\n",
        "    shapefiles_data[\"UnionPolygons\"],\n",
        "    style_function=lambda x: {\n",
        "        'fillColor': colormap(x['properties'][\"ageNum\"]),\n",
        "        'color': 'Black',\n",
        "        'weight': 2,\n",
        "        'fillOpacity': 0.7\n",
        "    },\n",
        "    tooltip=tooltip_shp,\n",
        "    popup=popup_shp,\n",
        ").add_to(interactive_map)\n",
        "\n",
        "# Create Popup template for locations\n",
        "popup_location = folium.GeoJsonPopup(\n",
        "    fields=[\"Age\", 'Easting', 'Northing'],\n",
        "    aliases=[\"Age\", 'Easting', 'Northing'],\n",
        "    localize=True,\n",
        "    labels=True,\n",
        "    style=\"background-color: yellow;\",\n",
        ")\n",
        "\n",
        "tooltip_location = folium.GeoJsonTooltip(\n",
        "    fields=[\"Age\", 'Easting', 'Northing'],\n",
        "    aliases=[\"Age: \", ' Easting: ', ' Northing: '],\n",
        "    localize=True,\n",
        "    sticky=False,\n",
        "    labels=True,\n",
        "    style=\"\"\"\n",
        "        background-color: #F0EFEF;\n",
        "        border: 2px solid black;\n",
        "        border-radius: 3px;\n",
        "        box-shadow: 3px;\n",
        "    \"\"\",\n",
        "    max_width=800,\n",
        ")\n",
        "\n",
        "# Age by location layer\n",
        "folium.GeoJson(\n",
        "    df_point,\n",
        "    name = \"Ages at Location\",\n",
        "    marker=folium.Circle(radius=1500, color = 'Black', fill_opacity = 1, fillColor = 'Black'),\n",
        "    style_function=lambda x: {\n",
        "        'fillColor': colormap(x['properties'][\"Age\"]), \n",
        "        'color': colormap(x['properties'][\"Age\"]),\n",
        "        'radius': 1500\n",
        "    },\n",
        "    tooltip=tooltip_location,\n",
        "    popup=popup_location,\n",
        ").add_to(interactive_map)\n",
        "\n",
        "\n",
        "# Add layers to map\n",
        "colormap.add_to(interactive_map)\n"
      ],
      "id": "1a50a5b8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Omitting plotting code here due to size\n",
        "interactive_map"
      ],
      "id": "0f2fb3ae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the interactive `folium` map, we can see that we have some polygons and data for about half of the boundary area.We have points outside of the boundary area, which is great as it gives us more information that will be helpful when making predictions.\n",
        "\n",
        "Preliminary trends indicate the ages of the samples get younger as we move eastward, This suggests that the general direction of magma movement was towards the east.\n",
        "\n",
        "Let's add some additional features and see what else we can see.\n",
        "\n",
        "### Feature Engineering (Part 1)\n",
        "\n",
        "We will now perform the first set of **feature engineering** on our data. This involves converting Easting and Northing coordinates to longitude and latitude for ease of interpretation, and creating quartiles for these coordinates. Additionally, we will incorporate important spatial features such as elevation and the geological period of the polygon each point falls into, with the periods one-hot encoded for scaling prior to model training.\n",
        "\n",
        "> **IMPORTANT !!!** The function `get_elevation_batch()` being used to get the elevation needs to be modified to run. Please go to the Running the Code section of the [README](https://github.com/lilynorthcutt/sierraNevadaAge?tab=readme-ov-file#running-the-code) for more information.\n"
      ],
      "id": "0235838b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Feature Engineering - CSV File\n",
        "# ---------- Lat/Lng -----------------\n",
        "lat, long = utm.to_latlon(df_ages['Easting'], df_ages['Northing'], 11, 'S')\n",
        "df_ages['lat'] = lat\n",
        "df_ages['long'] = long\n",
        "\n",
        "####### GEOLOGIC FEATURES #############\n",
        "\n",
        "# ----------- Elevation ---------------\n",
        "# Sending batch is faster than individual (https://github.com/sgherbst/pyhigh#)\n",
        "#### NOTE: need to change get_url_for_zip() function in env/lib/python3.9/pyhigh/elevation.py to\n",
        "# def get_url_for_zip(zip_name):\n",
        "#    return f'https://firmware.ardupilot.org/SRTM/North_America/{zip_name}'\n",
        "# Described in issue here https://github.com/sgherbst/pyhigh/issues/1\n",
        "coord_batch = list(zip(df_ages['lat'], df_ages['long']))\n",
        "df_ages['elevation'] = get_elevation_batch(coord_batch)\n",
        "\n",
        "# ----------- Add Period of Corresponding Polygon --------------\n",
        "# NOTE: This will limit all predictions to within the border if used\n",
        "\n",
        "# Create POINT() by easting and northing\n",
        "geometry = [Point(xy) for xy in zip(df_ages['Easting'], df_ages['Northing'])]\n",
        "df_ages = gpd.GeoDataFrame(df_ages, geometry=geometry, crs=\"EPSG:26711\")\n",
        "df_ages['period'] = \"None\"\n",
        "# Find if the point is within a polygon, and if so, assign it the corresponding period\n",
        "for idx, point in df_ages.iterrows():\n",
        "    for period, polygon in shapefiles_data['UnionPolygons'].iterrows():\n",
        "        if point.geometry.intersects(polygon.geometry):\n",
        "            df_ages.at[idx, 'period'] = period\n",
        "            break  # Stop checking once the polygon is found\n",
        "            \n",
        "# Perform one hot encoding on period \n",
        "df_ages['period_onehot'] = df_ages['period']\n",
        "df_ages = pd.get_dummies(df_ages, columns=['period_onehot'])\n"
      ],
      "id": "9954951b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can look the number of points that fall into each period below (note that one point does into the KJ period, its just hard to see).\n"
      ],
      "id": "44735925"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Look at the amount of points at each age in each period\n",
        "period = df_ages['period'].unique()\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=len(period), figsize=(15,5))\n",
        "plt.setp(ax, xlim=(80,130), ylim=(0, 16) )\n",
        "for i, ax in enumerate(ax.flat):\n",
        "    df = df_ages[df_ages['period'] == period[i]]\n",
        "    \n",
        "    ax.hist(df['Age'], bins=10, alpha=0.7, stacked=False)\n",
        "    ax.set_xlabel('Age (Ma)')\n",
        "    ax.set_ylabel('Count')\n",
        "    ax.set_title(f'Period: {period[i]}')\n",
        "    ax.grid(True)"
      ],
      "id": "d19685c9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we saw in the folium map, a lot of points are outside of the boundary region and do not fall into a polygon (i.e. `period == \"None\"`). But there are a decent amount of points in the eK and lK periods (only 1 in KJ).\n",
        "\n",
        "Next we can look at how the location features (Easting, Northing, and Elevation) relate to age.\n"
      ],
      "id": "dba23314"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plot all features with age individually\n",
        "list_to_plot = [\"Easting\", \"Northing\", \"elevation\"]\n",
        "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15,5))\n",
        "\n",
        "for i, ax in enumerate(ax.flat):\n",
        "    ax.scatter(df_ages[list_to_plot[i]], df_ages[\"Age\"], alpha=0.7, color='blue')\n",
        "    ax.set_xlabel(f'{list_to_plot[i]}')\n",
        "    ax.set_ylabel('Age (Ma)')"
      ],
      "id": "d171e8cf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plot all features together in 3D\n",
        "fig = px.scatter_3d(df_ages, x='Easting', y='Northing', z='elevation', color='Age',\n",
        "                    title = \"Age Distribution with Elevation (interact with me!)\")\n",
        "fig.update_layout(scene_zaxis_type=\"log\")"
      ],
      "id": "53643bea",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the graphs of relating easting, northing, and elevation to age we see that easting and elevation are showing a stronger trend with age than northing.\n",
        "\n",
        "### Test/Train Split\n",
        "\n",
        "Before training ML models, the data must be split into test and training sets. This is my first time working with GIS data and I was excited to learn about various test/train split methods when handling this sort of data. In addition to the traditional split, I tried out a Spatial KFold CV method, and a Cluster-Based Split method.\n"
      ],
      "id": "cae3134f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Split data into test and train \n",
        "\n",
        "# ----------- Traditional Split -----------------\n",
        "# Use if spatial autocorrelation is not significant concern\n",
        "train_trad, test_trad = train_test_split(df_ages, test_size=0.2, random_state=42)\n",
        "\n",
        "# ----------- Spatial K-Fold CV -----------------\n",
        "# Ensures spatially close points are either in training or testing (not both)\n",
        "coordinates = df_ages[['lat', 'long']].values\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for train_index, test_index in kf.split(coordinates):\n",
        "        train_kf, test_kf = df_ages.iloc[train_index], df_ages.iloc[test_index]\n",
        "\n",
        "# ----------- Cluster-Based Split ---------------\n",
        "# Cluster data points and then split each cluster into test/train\n",
        "kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "df_ages['cluster'] = kmeans.fit_predict(df_ages[['lat', 'long']])\n",
        "\n",
        "train_clust, test_clust = train_test_split(df_ages, test_size=0.2, stratify=df_ages['cluster'], random_state=42)\n",
        "\n",
        "################################################################################\n",
        "# Let's create a dictionary with the different test/train splits for convenience!\n",
        "df_dict = {\n",
        "    'trad': {'train': train_trad,'test': test_trad},\n",
        "    'kf': {'train': train_kf,'test': test_kf},\n",
        "    'cluster': {'train': train_clust,'test': test_clust}\n",
        "}"
      ],
      "id": "b0fb732e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The difference in how the test train splitting methods split the data with respect to location:\n"
      ],
      "id": "6c81c8d2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | include: false\n",
        "splits = [\n",
        "    {'name': 'Traditional','train': train_trad, 'test': test_trad},\n",
        "    {\"name\": \"Spatial K-Fold\",'train': train_kf, 'test': test_kf},\n",
        "    {\"name\":\"Cluster-Based\", 'train': train_clust, 'test': test_clust}\n",
        "]\n",
        "\n",
        "# View difference in test/train split for each method\n",
        "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15,5))\n",
        "for i, ax in enumerate(ax.flat):\n",
        "    name = splits[i]['name']\n",
        "    train_df = splits[i]['train']\n",
        "    test_df = splits[i]['test']\n",
        "    \n",
        "    ax.scatter(train_df['long'], train_df['lat'], color='black', label='Train Data', alpha=0.7)\n",
        "    ax.scatter(test_df['long'], test_df['lat'], color='red', label='Test Data', alpha=0.7)\n",
        "    ax.set_xlabel('Longitude')\n",
        "    ax.set_ylabel('Latitude')\n",
        "    ax.set_title(f' {name} Splits')\n",
        "    ax.grid(True)\n",
        "    ax.legend()\n"
      ],
      "id": "2790c980",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The difference in how the test train splitting methods split the data with respect to capturing age:\n"
      ],
      "id": "20e0bcc7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | echo: false\n",
        "# View distribution of ages in test/train partitions for each method\n",
        "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15,5))\n",
        "for i, ax in enumerate(ax.flat):\n",
        "    name = splits[i]['name']\n",
        "    train_df = splits[i]['train']\n",
        "    test_df = splits[i]['test']\n",
        "    \n",
        "    ax.hist([train_df['Age'], test_df['Age']], bins=10, color=['black', 'red'], label=['Train Data', 'Test Data'], alpha=0.7, stacked=True)\n",
        "    ax.set_xlabel('Age (Ma)')\n",
        "    ax.set_ylabel('Count')\n",
        "    ax.set_title(f'{name} Splits')\n",
        "    ax.grid(True)"
      ],
      "id": "ab03ffde",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | echo: false\n",
        "#\n",
        "plt.hist([test_trad['Age'], test_kf['Age'], test_clust['Age']], bins=10, color=['#00BE67', '#FF68A1', '#00A9FF'], label=['Traditional Split', 'Spatial K-Fold Split', 'Cluster-Based Split'], alpha=0.7, stacked=False)\n",
        "plt.xlabel('Age (Ma)')\n",
        "plt.ylabel('Count')\n",
        "plt.title(f'Age Count in Test for each Split Method')\n",
        "plt.grid(True)\n",
        "plt.legend( title='Test Age Count', loc='upper right')\n",
        "\n",
        "plt.show()"
      ],
      "id": "3817c111",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ultimately, I found that these advanced splitting methods did not significantly change the spatial representation or age distribution of my data compared to the traditional split. Additionally, I was concerned that these methods might not accurately represent my data for future predictions and could potentially lead to an overestimation of model accuracy by reducing the randomness in the splitting process. As a result, I will train the models with the traditionally split data.\n",
        "\n",
        "As I learn more about ML with GIS data, I may revisit this. Please reach out if you have tips/advice!\n",
        "\n",
        "### Cross Validation\n",
        "\n",
        "To ensure robust model evaluation for hyper-parameter tuning, I perform 5-fold cross-validation on my training set and store the results in a dictionary. I prefer using a dictionary for this purpose as it keeps everything organized and easily accessible!\n"
      ],
      "id": "2ac100a0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setting up k-fold cross-validation, and storing split in dictionary for easy access\n",
        "kf = KFold(n_splits=5) #5-folds\n",
        "for key in df_dict.keys():\n",
        "    # Take training data set for test/train split method(traditional, cluster, kfold)\n",
        "    train = df_dict[key]['train']\n",
        "    \n",
        "    # Split data into the 5 folds from TRAIN\n",
        "    # Store in a dictionary (cv) with all test and train splits\n",
        "    cv = {}\n",
        "    for i, (train_index, test_index) in enumerate(kf.split(train)):\n",
        "        cv[f'fold_{i}'] = {\n",
        "        'train': train.iloc[train_index],\n",
        "        'test': train.iloc[test_index],\n",
        "        'train_idx': train_index,\n",
        "        'test_idx': test_index\n",
        "    }\n",
        "    # Add the cv dictionary to the original df_dict dictionary with all of the test/train splits\n",
        "    df_dict[key]['cv'] = cv\n"
      ],
      "id": "cd0275c3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Second Round of Feature Engineering\n",
        "\n",
        "Within the cross-validation sets, I perform a second round of feature engineering to prevent data leakage and preserve the integrity of the model's predictions. It's crucial to avoid allowing the models to see the testing data, even within cross-validation folds.\n",
        "\n",
        "**Why not let `GridSearchCV()` split the data into k-folds?**\n",
        "\n",
        "While it might seem convenient to let `GridSearchCV` handle data splitting later on, there's an important reason for my approach. We already have some good features, but it's vital to provide the data with additional context about its surroundings. Specifically, for a given point `o_0`, I calculate the age, distance, and angle of the next closest points: `o_1`, `o_2`, and `o_3.` This is done in two dimensions (combining Easting and Northing) and individually for Easting, Northing, and Elevation in one dimension.\n",
        "\n",
        "**Preventing Leakage and Overfitting**\n",
        "\n",
        "A major concern in this process is data leakage, which occurs when the training data gets insights from the testing data, potentially leading to overfitting. To prevent this, we add these features only after all splitting has been completed:\n",
        "\n",
        "-   Training Data: The training data will have age/distance/bearing labels based on the training data itself.\n",
        "-   Test Data: Both the hold-out and cross-validation test sets will have age/distance/bearing labels based on the training data only.\n",
        "\n",
        "**Using `GridSearchCV()` for Hyper-Parameter Tuning** For hyper-parameter tuning of most models, I use `GridSearchCV()`. To integrate the custom feature engineering steps, I define a custom cross-validation (CV) class that uses the predefined CV indices along with the training set. This class adds the new features to the training and testing folds separately before training the models.\n",
        "\n",
        "**Note 2:** For the functions used in feature engineering, please refer to the full code provided at the link above.\n"
      ],
      "id": "75ddf37e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | include: false\n",
        "def calculate_bearing(point1, point2):\n",
        "    lat1 = np.radians(point1.y)\n",
        "    lon1 = np.radians(point1.x)\n",
        "    lat2 = np.radians(point2.y)\n",
        "    lon2 = np.radians(point2.x)\n",
        "    dlon = lon2 - lon1\n",
        "    x = np.sin(dlon) * np.cos(lat2)\n",
        "    y = np.cos(lat1) * np.sin(lat2) - (np.sin(lat1) * np.cos(lat2) * np.cos(dlon))\n",
        "    initial_bearing = np.arctan2(x, y)\n",
        "    initial_bearing = np.degrees(initial_bearing)\n",
        "    compass_bearing = (initial_bearing + 360) % 360\n",
        "    \n",
        "    return compass_bearing\n",
        "  \n",
        "def get_closest_points_metrics(selected_point, reference_df, num_closest=3, training_point = True):\n",
        "    '''\n",
        "    Function to calculate the closest points metrics in 1D, 2D and 3D, and assigning the ages, distances, and directions\n",
        "    to the num_closest points\n",
        "    :param selected_point: geopandas df with 1 row, where selected_point is the point from which we want to find the closest points\n",
        "    :param reference_df: this is the reference of the points we are looking at and comparing selected_point to\n",
        "    :param num_closest: the top number of closest points we care about\n",
        "    :param training_point: boolean to indicate if we are looking at training points or not, i.e. if selected_point came from reference_df\n",
        "    :return: Returns a pd.series equal to selected_point with the addition of the num_closest points age,direction, and bearing in 1D and 2D\n",
        "    '''\n",
        "    \n",
        "    # If we are labeling the training set, then selected_point will be in points_gdf, so we want to remove it\n",
        "    if training_point:\n",
        "        remaining_points_gdf = reference_df.drop(index=selected_point.name)\n",
        "    else:\n",
        "        remaining_points_gdf = reference_df\n",
        "     \n",
        "    # Calculate and save the ages, distances, and bearing of num_closest points closest to selected_point\n",
        "    # for each dimension (easting, northing, elevation) and for 2dimension top down view (easting and northing combined\n",
        "    metrics = {}\n",
        "    ##################\n",
        "    #### 1 Dimension\n",
        "    dims = ['Northing', 'Easting', 'elevation']\n",
        "    \n",
        "    for dim in dims:\n",
        "        # Calculate distances\n",
        "        remaining_points_gdf[f'distance_1d_{dim.lower()}'] = remaining_points_gdf.apply(lambda row: \n",
        "                                                    abs(row[dim] - selected_point[dim]), axis = 1)\n",
        "        # Calculate bearings (its 1D so im just doing: -1,0,1)\n",
        "        remaining_points_gdf[f'bearing_1d_{dim.lower()}'] = remaining_points_gdf.apply(lambda row: 0 if row[f'distance_1d_{dim.lower()}'] == 0 else\n",
        "                                                   (row[dim] - selected_point[dim])/abs(row[dim] - selected_point[dim]), axis = 1)\n",
        "        # sort the data based on distance\n",
        "        sorted_points_gdf = remaining_points_gdf.sort_values(by=f'distance_1d_{dim.lower()}').head(num_closest)\n",
        "        # Assign points to the metric dictionary\n",
        "        for i, (_, row) in enumerate(sorted_points_gdf.iterrows(), start=1):\n",
        "            metrics[f'age_1d_{dim.lower()}_{i}'] = row['Age']\n",
        "            metrics[f'distance_1d_{dim.lower()}_{i}'] = row[f'distance_1d_{dim.lower()}']\n",
        "            metrics[f'bearing_1d_{dim.lower()}_{i}'] = row[f'bearing_1d_{dim.lower()}']\n",
        "\n",
        "    \n",
        "    ###################\n",
        "    #### 2 Dimension\n",
        "    remaining_points_gdf['distance_2d'] = remaining_points_gdf.geometry.distance(selected_point.geometry)\n",
        "    remaining_points_gdf['bearing_2d'] = remaining_points_gdf.geometry.apply(lambda x: calculate_bearing(selected_point.geometry, x))\n",
        "    \n",
        "    sorted_points_gdf = remaining_points_gdf.sort_values(by='distance_2d').head(num_closest)\n",
        "    \n",
        "    for i, (_, row) in enumerate(sorted_points_gdf.iterrows(), start=1):\n",
        "        metrics[f'age_2d_{i}'] = row['Age']\n",
        "        metrics[f'distance_2d_{i}'] = row['distance_2d']\n",
        "        metrics[f'bearing_2d_{i}'] = row['bearing_2d']\n",
        "    \n",
        "    \n",
        "    return pd.Series(metrics)"
      ],
      "id": "0be6c2b1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Post Split Feature Engineering - Only using training data as reference\n",
        "\n",
        "# <--------- Closest n Ages + Direction + Bearing ------------->\n",
        "# We will calculate age of the closest 3 points in for each direction individually (northing/easting/elevation),\n",
        "# and top-down in 2 dimensions (easting+northing): Adding age, direction and bearing for each\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# For each method of splitting, for each cross-validation training set calculate the above\n",
        "for key in df_dict.keys():\n",
        "    ### Full Testing ###\n",
        "    df = df_dict[key]['test']\n",
        "    ref = df_dict[key]['train']\n",
        "    df = df.apply(lambda row: pd.concat([row, get_closest_points_metrics(\n",
        "      selected_point=row, reference_df= ref,  training_point=False)]), axis=1)\n",
        "    df_dict[key]['test'] = df\n",
        "    \n",
        "    for fold in df_dict[key]['cv']:\n",
        "        ### K-Fold Training ###\n",
        "        # Assign the data we want to make calculations on \n",
        "        df = df_dict[key]['cv'][fold]['train']\n",
        "        # Call function on each row\n",
        "        # NOTE: Because we are labeling training data with training data we need to specify this to the function\n",
        "        # so that it can remove this point from the reference dataset\n",
        "        # (this is needed because when we label the test data we use training data as the reference and won't be removing the point\n",
        "        df = df.apply(lambda row: pd.concat([row, get_closest_points_metrics(\n",
        "          selected_point=row, reference_df= df, training_point=True)]), axis=1)\n",
        "        df_dict[key]['cv'][fold]['train'] = df\n",
        "        \n",
        "        ### K-Fold Testing ###\n",
        "        # Assign the data we want to make calculations on \n",
        "        df = df_dict[key]['cv'][fold]['test']\n",
        "        # Ref is training set (same as before)\n",
        "        # Training_point = False this time\n",
        "        df = df.apply(lambda row: pd.concat([row, get_closest_points_metrics(\n",
        "          selected_point=row, reference_df= ref, training_point=False)]), axis=1)\n",
        "        df_dict[key]['cv'][fold]['test'] = df\n",
        "\n",
        "\n",
        "end = time.time()\n",
        "print('Time taken to calculate metrics: ' + str(end - start) + \" seconds\")"
      ],
      "id": "ad7e1359",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class CustomCVSplitter(BaseCrossValidator):\n",
        "    \n",
        "    def __init__(self, cv_indices, feature_list):\n",
        "        self.cv_indices = cv_indices\n",
        "        self.feature_list = feature_list\n",
        "        \n",
        "    def split(self, X, y=None, groups=None):\n",
        "        # Define the folds from the indices given\n",
        "        for fold, (train_idx, test_idx) in enumerate(self.cv_indices):\n",
        "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "\n",
        "            # Apply feature engineering \n",
        "            X_train = X_train.apply(lambda row: pd.concat([row, get_closest_points_metrics(\n",
        "              selected_point=row, reference_df= X_train,  training_point=True)]), axis=1)\n",
        "            X_test = X_test.apply(lambda row: pd.concat([row, get_closest_points_metrics(\n",
        "              selected_point=row, reference_df= X_train, training_point=False)]), axis=1)\n",
        "        \n",
        "            # Remove Age from all X (age is target)\n",
        "            X_train = X_train[self.feature_list]\n",
        "            X_test = X_test[self.feature_list]\n",
        "\n",
        "            yield train_idx, test_idx\n",
        "            \n",
        "        \n",
        "        \n",
        "    def get_n_splits(self, X=None, y=None, groups=None):\n",
        "        return len(self.cv_indices)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.get_n_splits()\n"
      ],
      "id": "036712a3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | include: false\n",
        "def problem_columns(x):\n",
        "    return x.drop(columns=['Age', 'geometry'], errors='ignore')"
      ],
      "id": "f97f92c9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Models\n",
        "\n",
        "In this section, I will walk through the training process for two out of the five models I've trained on this data: a K-Nearest Neighbor (KNN) model and a Gradient Boosting Machine (GBM) model. For the KNN model, we will manually tune the parameters and select the best one. For the GBM model, we will demonstrate how to use the custom cross-validation and pipeline functionality with `GridSearchCV()`.\n",
        "\n",
        "The full list of models trained in this project includes:\n",
        "\n",
        "-   Decision Tree\n",
        "-   Random Forest\n",
        "-   Gradient Boosting Machine (GBM)\n",
        "-   K-Nearest Neighbor (KNN)\n",
        "-   Gaussian Process Regressor The full code, including training for all models, is available at the link provided at the top.\n",
        "\n",
        "Let's get started!\n",
        "\n",
        "Before diving into the specific models, let's define and initialize a few key components to ensure the training process runs smoothly.\n"
      ],
      "id": "a3a6a374"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# For now I'm just training on the traditionally split test/train data - and will repeat on the other splits at a later time\n",
        "df_test_train = df_dict['trad']\n",
        "\n",
        "# Set list of features and target\n",
        "target = list([\"Age\"])\n",
        "list_features = list(['Easting', 'Northing', 'elevation', 'period_onehot_KJ', 'period_onehot_None', 'period_onehot_eK', 'period_onehot_lK', \n",
        "                      'age_1d_northing_1', 'distance_1d_northing_1', 'bearing_1d_northing_1',\n",
        "                      'age_1d_northing_2', 'distance_1d_northing_2', 'bearing_1d_northing_2',\n",
        "                       'age_1d_northing_3', 'distance_1d_northing_3', 'bearing_1d_northing_3',\n",
        "                       'age_1d_easting_1', 'distance_1d_easting_1', 'bearing_1d_easting_1',\n",
        "                       'age_1d_easting_2', 'distance_1d_easting_2', 'bearing_1d_easting_2',\n",
        "                       'age_1d_easting_3', 'distance_1d_easting_3', 'bearing_1d_easting_3',\n",
        "                       'age_1d_elevation_1', 'distance_1d_elevation_1',\n",
        "                       'bearing_1d_elevation_1', 'age_1d_elevation_2',\n",
        "                       'distance_1d_elevation_2', 'bearing_1d_elevation_2',\n",
        "                       'age_1d_elevation_3', 'distance_1d_elevation_3',\n",
        "                       'bearing_1d_elevation_3', 'age_2d_1', 'distance_2d_1', 'bearing_2d_1',\n",
        "                       'age_2d_2', 'distance_2d_2', 'bearing_2d_2', 'age_2d_3',\n",
        "                       'distance_2d_3', 'bearing_2d_3'])\n",
        "# Why do we include age? because we need it in order to add features within our custom_cv - we will remove age though so that it is not in our training set\n",
        "list_features_pre_split = list(['Age','geometry','Easting', 'Northing', 'elevation', 'period_onehot_KJ', 'period_onehot_None', 'period_onehot_eK', 'period_onehot_lK'])\n",
        "\n",
        "# Set iterable yielding (train, test) splits as arrays of indices for GridSearchCV()\n",
        "df_test_train['cv'].keys()\n",
        "cv_idx_split = list([])\n",
        "\n",
        "for key in df_test_train['cv'].keys():\n",
        "    train_list = df_test_train['cv'][key]['train_idx'].tolist()\n",
        "    test_list = df_test_train['cv'][key]['test_idx'].tolist()\n",
        "    cv_idx_split.append((train_list, test_list))\n",
        "    \n",
        "    \n",
        "# Initialize a custom_cv\n",
        "custom_cv = CustomCVSplitter(cv_idx_split, list_features)\n",
        "    \n",
        "# Initialize model metrics\n",
        "df_test_train['model_metrics'] = {}"
      ],
      "id": "91278a2c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### K-Nearest Neighbors\n",
        "\n",
        "The k-nearest neighbors (KNN) algorithm is a versatile non-parametric supervised learning method that makes predictions based on the k nearest points to a given data point. The challenge lies in determining the optimal number of nearest points (`k`) to use. To start, we will try `k` values ranging from 1 to 20 and evaluate the average RMSE (Root Mean Squared Error) across the cross-validation folds for each `k.`\n"
      ],
      "id": "0045fb4f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ----------- Hyper-Parameter Tuning -------------\n",
        "max_k = 20\n",
        "\n",
        "# On each fold:\n",
        "#   => On each k (num neighbors):\n",
        "#       => train model \n",
        "#       => predict age in each test set with model (to make easier, make new dict {k: []} | for all k}\n",
        "fold_dict = {k: {\"train\": [], \"test\": []} for k in range(1, max_k+1)}\n",
        "for k in range (1, max_k+1):\n",
        "    # Set k\n",
        "    knn = KNeighborsRegressor(n_neighbors=k)\n",
        "    \n",
        "    # Train for each fold\n",
        "    for fold in df_test_train['cv'].keys():\n",
        "        # Set test and train x and y \n",
        "        X_train = df_test_train['cv'][fold]['train'][list_features]\n",
        "        y_train = df_test_train['cv'][fold]['train'][target]\n",
        "        X_test = df_test_train['cv'][fold]['test'][list_features]\n",
        "        y_test = df_test_train['cv'][fold]['test'][target]\n",
        "        \n",
        "        # Scale features\n",
        "        scaler = StandardScaler()\n",
        "        X_train = scaler.fit_transform(X_train)\n",
        "        X_test = scaler.transform(X_test)\n",
        "        \n",
        "        # Train knn model \n",
        "        knn.fit(X_train, y_train) \n",
        "        \n",
        "        # Predict test\n",
        "        pred_test = knn.predict(X_test)\n",
        "        pred_train = knn.predict(X_train)\n",
        "        \n",
        "        # Calculate RMSE\n",
        "        rmse_test = np.sqrt(mean_squared_error(y_test, pred_test))\n",
        "        rmse_train = np.sqrt(mean_squared_error(y_train, pred_train))\n",
        "        fold_dict[k]['test'].append(rmse_test)\n",
        "        fold_dict[k]['train'].append(rmse_train)\n"
      ],
      "id": "0ef57cd3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's plot the RMSE values for different `k` neighbors and choose the best `k` based on the \"elbow\" of the test RMSE curve. From the graph below, it appears that the optimal `k` is 4.\n"
      ],
      "id": "f8a21696"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------------- Select Model ------------------\n",
        "test = list([])\n",
        "train = list([])\n",
        "for k in fold_dict.keys():\n",
        "   train.append(np.mean(fold_dict[k]['train'])) \n",
        "   test.append(np.mean(fold_dict[k]['test']))\n",
        "   \n",
        "plt.plot(train, c='b', label='train')\n",
        "plt.plot(test, c='r', label='test')\n",
        "plt.legend()\n",
        "plt.title(\"KNN: Average RMSE over 5-folds\")\n",
        "plt.xlabel(\"k Neighbors\")\n",
        "plt.ylabel(\"RMSE\")\n",
        "plt.show()\n",
        "# We want to select k where the \"elbow\" in the below graph appears.\n",
        "# It looks like this is when k = 4\n",
        "k_bestmodel = 4"
      ],
      "id": "eebe33a2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the optimal `k` determined, we can now train the KNN model on the entire training set using `k=4`.\n",
        "\n",
        "It's important to add the closest point metric features to the training set at this stage. The reason for this is that, unlike the other models which will use the training set in `GridSearchCV` for parameter tuning, we are manually tuning the KNN model. If we had added these features earlier, it would have introduced information about the test set into the training set, leading to data leakage when we use `GridSearchCV`.\n"
      ],
      "id": "767cf23a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ------- Train Model on all Training Data -------\n",
        "# Now that we chose our parameter as 5, we will train and test on the full training and test set\n",
        "knn = KNeighborsRegressor(n_neighbors=k_bestmodel)\n",
        "# Set test and train x and y \n",
        "X_train = df_test_train['train'].apply(lambda row: pd.concat([row, get_closest_points_metrics(selected_point=row, reference_df= df_test_train['train'], num_closest=3, training_point=True)]), axis=1)\n",
        "X_train = X_train[list_features]\n",
        "y_train = df_test_train['train'][target]\n",
        "X_test = df_test_train['test'][list_features]\n",
        "y_test = df_test_train['test'][target]\n",
        "        \n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "        \n",
        "# Train knn model \n",
        "knn.fit(X_train, y_train) \n",
        "        \n",
        "# Predict test\n",
        "pred_test = knn.predict(X_test)\n",
        "pred_train = knn.predict(X_train)\n",
        "        \n",
        "# Calculate RMSE\n",
        "rmse_test = np.sqrt(mean_squared_error(y_test, pred_test))\n",
        "# Calculate r2\n",
        "r2_test = r2_score(y_test, pred_test)\n",
        "print(\"R2: \"+ str(r2_test))\n",
        "\n",
        "#### Add metrics to dictionary\n",
        "df_test_train['model_metrics']['knn'] = {'model':knn, 'name': \"knn\",'rmse': rmse_test, 'r2': r2_test,'predicted_test': pred_test}\n",
        "\n",
        "\n",
        "#### Plot \n",
        "plt.scatter(y_test, pred_test, color='b')\n",
        "plt.plot(y_test, y_test, color = 'black')\n",
        "plt.title(\"KNN: Test Age vs. Predicted Test Age\")\n",
        "plt.xlabel(\"Age (Ma)\")\n",
        "plt.ylabel(\"Predicted Age (Ma)\")    \n",
        "\n",
        "plt.show()"
      ],
      "id": "489aaca6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gradient Boosting Machine\n",
        "\n",
        "For the GBM model, we will use `GridSearchCV` combination with the custom cross-validation class defined earlier and pipeline defined below In our parameter grid, each parameter name must be prefixed with `model__` to ensure that `GridSearchCV` correctly applies the parameters to the appropriate part of the pipeline. This setup allows us to systematically search for the best parameters while leveraging the benefits of cross-validation.\n"
      ],
      "id": "30f16585"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train model on all folds (indices for our cv already specified in grid_search)\n",
        "X_train = df_test_train['train'][list_features_pre_split]\n",
        "y_train = df_test_train['train'][target]\n",
        "X_test = df_test_train['test'][list_features]\n",
        "y_test = df_test_train['test'][target]\n",
        "\n",
        "\n",
        "# ----------- Hyper-Parameter Tuning -------------\n",
        "param_grid = {\n",
        "    'model__n_estimators': [50, 100, 200],\n",
        "    'model__max_depth': [3, 5, 7],\n",
        "    'model__min_samples_split':[2,4,6,8,10,20,40,60,100], \n",
        "    'model__min_samples_leaf':[1,3,5,7,9]\n",
        "}\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('remove_columns', FunctionTransformer(problem_columns)),\n",
        "    ('model', GradientBoostingRegressor())\n",
        "])\n",
        "\n",
        "model = 'gradient_boosting_regressor'\n",
        "\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=custom_cv, scoring='neg_mean_squared_error', n_jobs=-1,\n",
        "                           error_score='raise').fit(X_train, y_train.values.ravel())\n",
        "\n",
        "grid_search"
      ],
      "id": "2b549076",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can select the best parameters below:\n"
      ],
      "id": "0af09855"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------------- Select Model ------------------\n",
        "best_params = grid_search.best_params_\n",
        "print(best_params)"
      ],
      "id": "5ea982f7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the best parameters identified, we can retrain the GBM model on the full training set using these parameters. Finally, we will make predictions on the test set and evaluate the models performance.\n"
      ],
      "id": "4318d12b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ------- Train Model on all Training Data -------\n",
        "# Add features to full training set:\n",
        "X_train = X_train.apply(lambda row: pd.concat([row, get_closest_points_metrics(selected_point=row, reference_df= X_train, \n",
        "                                                                             num_closest=3, training_point=True)]), axis=1)\n",
        "X_train = X_train[list_features]\n",
        "\n",
        "# Fit rf with the full training dataset\n",
        "best_pipeline = Pipeline([\n",
        "    ('remove_columns', FunctionTransformer(problem_columns)),\n",
        "    ('model', GradientBoostingRegressor(**{k.replace('model__', ''): v for k, v in best_params.items()}))\n",
        "])\n",
        "best_pipeline.fit(X_train, y_train.values.ravel())\n",
        "\n",
        "# Predict test\n",
        "pred_test = best_pipeline[1].predict(X_test)\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse_test = np.sqrt(mean_squared_error(y_test, pred_test))\n",
        "# Calculate r2\n",
        "r2_test = r2_score(y_test, pred_test)\n",
        "print(\"R2: \"+ str(r2_test))\n",
        "\n",
        "\n",
        "\n",
        "#### Add metrics to dictionary\n",
        "df_test_train['model_metrics']['gbm'] = {'model':best_pipeline[1], 'name': model,'rmse': rmse_test, 'r2': r2_test,'predicted_test': pred_test}\n",
        "\n",
        "\n",
        "#### Plot \n",
        "plt.scatter(y_test, pred_test, color='b')\n",
        "plt.plot(y_test, y_test, color = 'black')\n",
        "plt.title(\"GBM: Test Age vs. Predicted Test Age\")\n",
        "plt.xlabel(\"Age (Ma)\")\n",
        "plt.ylabel(\"Predicted Age (Ma)\")    \n",
        "\n",
        "plt.show()"
      ],
      "id": "9bfa779e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate\n",
        "\n",
        "After training all our models, we can assess their performance using metrics such as RMSE (Root Mean Squared Error) and R\\^2. The following table summarizes these metrics for each model:\n"
      ],
      "id": "7becfca1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | echo: false\n",
        "evalTable = PrettyTable([\"Model\", \"RMSE\", \"R2\"])\n",
        "\n",
        "evalTable.add_row([\"Decision Tree\", 6.78, 0.57])\n",
        "evalTable.add_row([\"Random Forest\", 5.82, 0.68])\n",
        "evalTable.add_row([\"GBM\", 5.26, 0.74])\n",
        "evalTable.add_row([\"KNN\", 5.38, 0.73])\n",
        "evalTable.add_row([\"Gaussian Process Regressor\", 4.55, 0.81])\n",
        "\n",
        "print(evalTable)"
      ],
      "id": "b44a58c9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the table, we can see that the Gaussian Process Regressor is the best-performing model, with an **R2 score of 0.81**. This indicates that the Gaussian Process Regressor explains a significant portion of the variance in the data.\n",
        "\n",
        "To gain further insights into the models, we can explore feature importance. For models like Decision Trees, Random Forests, and Gradient Boosting Machines (GBM), we can use built-in methods to obtain feature importance scores. Otherwise, we use permutation importance to assess the impact of each feature on model performance. I won't be addressing this here as I only trained two of the models in this writeup.\n",
        "\n",
        "For practical applications, including integration into the Shiny app, we will retrain the selected model (Gaussian Process Regressor with the best parameters) using all available data. This final model will be used for predicting and forecasting unknown data points.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This project successfully developed and applied new machine learning (ML) methods to address critical gaps in geologic maps by predicting undated rock ages within the Sierra Nevadas. By leveraging advanced algorithms, including Decision Trees, Random Forests, Gradient Boosting Machines, K-Nearest Neighbors, and Gaussian Process Regressors, we have provided valuable insights into magmatic migration and rock formation processes. The Gaussian Process Regressor emerged as the top performer, offering a robust model with an R2 score of 0.81.\n",
        "\n",
        "This approach allows us to present a less biased view of magmatic migration, enhancing our understanding of the geological history in the Sierra Nevadas.\n",
        "\n",
        "Additionally, the interactive visualizations in Shiny enable researchers and interested individuals to explore the data and predictions (predictions coming soon). These visualizations offer an intuitive and engaging way to examine geologic ages, model predictions, and spatial relationships, making the data more accessible and informative.\n",
        "\n",
        "## Future Work\n",
        "\n",
        "This was my first project working with GIS data, and I have lots of ideas for additionally avenues I would like to explore. Here are a few ideas:\n",
        "\n",
        "**Enhanced ML Methods:**\n",
        "\n",
        "-   Investigate more sophisticated ML models, including deep learning techniques and geo neural networks to improve accuracy and robustness\n",
        "-   Further optimization of model parameters through methods such as Bayesian optimization\n",
        "\n",
        "**Broadening Application Results:**\n",
        "\n",
        "-   Finding other use case areas to apply the methodology explored here, and compare results to see if the modeling can be applied universally or only at this location\n",
        "\n",
        "**Improving Shiny Visualizations:**\n",
        "\n",
        "-   Add Models:Add grid of boundary area predicted by models to show ages predicted uniformly over the area\n",
        "-   Age Histogram: Add histogram of ages corresponding to the ages in the map (i.e. lines up with the Age sliderbar)\n",
        "-   Custom Line Predictions: Add functionality for users to select a model, and select a line on the map. The app will then generate predictions of ages along the line and output a 2D graph of *line* vs. age.\n",
        "\n",
        "Thank you for stopping by and reading through my work!\n",
        "\n",
        "Please feel free to reach out with any comments or suggestions.\n",
        "\n",
        "."
      ],
      "id": "c657dd8c"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}